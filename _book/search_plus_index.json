{"./":{"url":"./","title":"简介","keywords":"","body":"简介 这是呼吸君的关于 Linux内核 个人学习笔记。 那么我们现在开始吧！ "},"实验一/要求.html":{"url":"实验一/要求.html","title":"实验一","keywords":"","body":"实验一的要求 三个实验： 编译一遍 Linux内核，输入uname -a即可看到新的内核。 添加书上的prink语句，输入dmesg可以得到自己添加的系统日志 完成自定义添加的系统调用，能解释每一条语句，并成功运行 "},"实验一/过程.html":{"url":"实验一/过程.html","title":"实验过程","keywords":"","body":"实验一 Linux内核编译及添加系统调用 书本332页： 课本给的环境是 ubantu64 位，但是老子不喜欢，我就是喜欢用 centos 系统。我喜欢用XShell的感觉。完成这个实验我建议使用自己的电脑安装虚拟机内使用，或者用自己的服务器。毕竟有些工作要编译好久。 我个人的系统是CentOS 6.8 64位,重装后开始啦！ 第一步，下载内核源码。 首先我们访问下载源码的官方网站：kernel.org ，可以看到现在最新的内核版本是 5.0.5 右键单击 黄色按钮（或者单击我），复制链接地址。 然后打开命令行界面，输入 wget 复制到的地址,按下enter,开始下载源码。（提示：Shift + insert 就是粘贴的快捷键） 当然你可以直接复制下面的命令直接使用 wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.0.5.tar.xz 开始下载 下载完成后输入 ll 或者 ls -l 就能看到下载好的文件 第二步 ，解压缩 下载完成后，依次输入下面的指令进行解压。 xz -d linux-5.0.5.tar.xz tar -xvf linux-5.0.5.tar 第一条指令没有任何提示，第二条有好多提示。 看，解压完成！ 接下来输入 cd linux-5.0.5 进入内核子目录。输入 ll或ls -l即可查看内核文件。 注意：这个地方就是内核的子目录，后面我们会多次提到。你可以输入pwd查看这个目录的位置。 第三步，清除残留的文件 按照书本的命令，此处当然行不通，所以我们先安装缺少的包。执行下面的指令： yum install ncurses-devel 输入 y 确认 完成后输入输入下面的指令 make mrproper 发现报错： 需要我们安装 gcc ，于是继续安装 yum -y install gcc 安装完成后重新输入 make mrproper 如下图无报错即可。 第四步、分配系统调用号，修改系统调用表 此处在书本 331页。 现在我们已经在内核的根目录下了，但是系统调用表并不在这个目录下。我们先移动到相应目录下： cd arch/x86/entry/syscalls/ 在此处输入ll就能看到所有的系统调用表。 但是具体用的是那个表呢，我们这里就需要查看自己的系统版本了，输入 uname -a即可查看。 其中显示 x86_64所以我们是64位的系统。 所以我们打开syscall_64.tbl这个表： vim syscall_64.tbl | 提示：按住 shift + g移动到结尾，按gg移动到开头。 我们按方向键查看数据，发现每个系统调用在表中占一个表项，我们模仿着这个也去在下面写一行 （提示：按一下 A 即可写入） 335 64 hlsyscall __x64_sys_hlsyscall 按 Esc ，然后输入 :wq即可保存并退出 第五步、申明系统调用服务例程原型 切换到内核的子目录下,然后打开以下路径: cd include/linux/ 这个目录下有许多头文件，我们使用Vim打开 syscalls.h vim syscalls.h 在文件末尾添加刚才的系统调用： asmlinkage long __x64_sys_hlsyscall(void); 最后 依次按下 Esc :wq Enter保存退出 第六步，实现系统调用服务例程 在添加了系统调用号（第四步）以及声明了系统调用（第五步）后，就可以写例程了。 切换到内核的子目录下,然后打开以下路径: cd kernel/ 这个目录下有许多头文件，我们使用Vim打开 sys.c vim sys.c 切换到文件结尾，开始仿照下面的代码编写： SYSCALL_DEFINE0(hlsyscall) { printk(\"Hello,this is hl's syscall test!\\n\"); return 0; } 写入后退出。 第七步、配置内核 以上步骤已经完成了 从0开始 到 添加一个新系统调用 的所有工作，但是现在还无法运行起来，我们需要重新编译内核。 首先切换到内核过根目录， 按照书本的指令，我们输入 make menuconfig 发现还是报错： 原来是缺少 flex，我们就安装flex: yum install flex 然后重新配置内核： make menuconfig 发现还是报错： 原来是缺少 bison ,我们继续安装： yum install bison 安装完成后又又重新配置内核： make menuconfig 这次成功啦！ 建议此时创建系统快照，接下来的操作不一样喽 到了这里我也看不懂要放啥，所以直接右键到 save让他使用默认配置就好。 这里 ok以后exit,再Exit即可 第八步、编译内核，生成启动镜像文件 查看一下自己的系统内核，虚拟机设置里面就能看到。我买的腾讯云服务器,只有一个cpu内核 所以编译时输入 make 即可，有多个内核可以 make -j2（双核)、或 make -j4(四核)加快编译速度。 开始编译。 我擦，又报错了： 检查错误，原来缺少 openssl ，安装！ yum install openssl 安装完成后，重新 make 发现未解决问题，那我们按照书上的来： yum install openssl 结果...还是不行，emmmm...，根据我之前安装java的经验，我觉得应该安装开发版，所以又执行 yum install openssl-devel 现在重新编译！ make 我去，但是接踵而来的又是新问题： 继续看，your compiler is too old - please upgrade it.原来是 编译器 太老了，需要更新编译器版本。 那么问题来了来了，之前我们就安装过 gcc 了，而且就是最新的版本，为啥还是说编译器不行呢？ 是的，那么答案就很明显了，我现在的系统内核没办法安装真正最新版的 gcc ! 所以接下来我们有三种解决办法： 第一种方法：编译低版本的linux内核。 第二种：使用 CentOS 7，更新完最新的GCC。 第三种：升级内核完升级GCC。 稍微查了下资料。发现 centos 6 使用的多为 linux2 版本，最新的centos 7使用的多为 linux3的版本，但是我却要编译 linux5 。那么这里也就解释了之前一处奇怪的地方：为什么我们的系统调用表和书上的不一样。三种方案之下，无疑最方便的解决方案是第一种：编译低版本的 linux内核。但是我是优秀的网安同学，我就是想编译 linux 5！所以，我接下来会创建一个快照，然后逐一演示三种方案。 这里你可能觉得我做的太麻烦了，为什么要演示三种方法呢？别着急，慢慢看，编译系统是一个好玩有趣的思考游戏。我会在最后将以上步骤编写到一个Shell,实现一键快速编译! 准备之前的工作： 我现在用的 CentOS 6，我们检查一下内核。 uname -r 发现我的内核才是 linux2 的版本。这样怎么编译 linux5 啊！ 再看看 gcc 版本，待会对比看看是不是这个问题~ gcc --version gcc 也是2012年的，7年前的 gcc ，我去，怪不得。 第一种方案：编译低版本的linux内核 我们重新访问下载源码的官方网站：kernel.org ，这次我们要选择 linux 4 的内核下载。 我看看，感觉这个4.14.109的版本比较顺眼，就它了！ 右键单击tarball复制下载地址，然后使用wget指令下载，哦，别忘了，删掉之前的 linux 5 内核。 rm -rf linux* wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.14.109.tar.xz 可惜腾讯云的带宽太不稳定，接下来我将在室友的虚拟机里继续演示。 虚拟机拥有文件共享功能，于是我用迅雷先下载到本地再传输到linux中。 继续解压 xz -d linux-5.0.5.tar.xz tar -xvf linux-5.0.5.tar 然后依次执行刚才的第三步，第四步，第五步，第六步。 在生成完 .config 后，我们就终于可以再次make了。 这次我们遇到了全新的报错：但还是缺少了libelf，所以继续安装！ yum install elfutils-libelf-devel 再次make！但是我可以告诉你，这一步以后就没问题了。相比于云服务器，虚拟机里面我们设置了CPU核的数量，所以这里编译用make -j +处理器核的数量编译，可以大大的加快速度。 比如室友的虚拟机有8内核，我们输入 make -j8 bzImage 然后开始编译啦！ 在经历了 20 多分钟后，编译成功了！ 接下来在内核子目录下编译模块： make modules 可以查看到有1702个模块被编译出来。为了便于稍后的对比，我们先来看下现在的内核： uname -a 继续安装内核。 make modules_install depmod mkinitramfs -o /boot/initrd-4.14.109.img -v 4.14.109 make install 这次继续报错： 百度以后，确认这个错误无任何影响~ 接下来直接reboot重启,在虚拟机里我们就能看到两个操作系统，选择更高的按enter进入。 在这之后，启动新内核遇到了问题，原因是我不知道 centos6 的 grub2 的文件在哪。不知道如何配置 grub 引导程序，所以暂停一下，下周问老师。 第二种方案：使用 CentOS 7，更新完最新的GCC 这种方法最适合云服务器了，直接选择最新的系统安装：CentOS7 安装完成后我们可以看看全新的内核： uname -r 安装完成后重新执行第一步到第七步，我们这次再检查 gcc 版本： gcc --version 果然更新到了 4.8.5！ 在生成完 .config 后，我们就终于可以再次make了。 然鹅这次我们遇到了全新的报错：但还是缺少了libelf，所以继续安装！ yum install elfutils-libelf-devel 再次make！这次我们虽然编译的是 linux5 ，但是估计没问题了，让他慢慢编译吧，我先睡中午觉去了，我单核的云服务器估计很慢~ 答案出来了，编译了两个小时！我都睡醒了~ 接下来去内核子目录下编译模块： make modules 可以查看到有2437个模块被编译出来。为了便于稍后的对比，我们先来看下现在的内核： uname -a 继续安装模块和内核。 make modules_install make install 接下来配置 grub 引导程序： grub2-mkconfig -o /etc/grub2.cfg 因为是云服务器，我们需要继续设置系统默认启动项： grub2-set-default 0 最后 reboot ! 进入系统后查看版本： uname -a ok啦，内核已经是最新的 5.0.5 啦！ 第九步、编写用户态程序测试新系统调用 新建一个目录，Vim 一个 .c文件（书本332页） vim mySyscall.c #define _GNU_SOURCE #include #include #define __NR_mysyscall 335 int main(){ syscall(335); } 保存退出后 gcc一下，随后会得到一个 a.out 的可执行文件，我们执行以下 gcc mySyscall.c ll ./a.out 最后使用 dmesg 即可查看输出内容： dmesg | tail -n 5 至此，实验一全部完成！ 实验报告传送门：点我 "},"实验一/实验报告.html":{"url":"实验一/实验报告.html","title":"实验报告","keywords":"","body":"实验一 Linux内核编译及添加系统调用 设计目的和内容要求 1.设计目的 Linux是开源操作系统，用户可以根据自身系统需要裁剪、修改内核，定制出功能更加合适、运行效率更高的系统，因此，编译 Linux内核是进行内核开发的必要基本功。在系统中根据需要添加新的系统调用是修改内核的一种常用手段，通过本次实验，读者应理解 Linux系统处理系统调用的流程以及增加系统调用的方法。 2.内容要求 （1）添加一个系统调用，实现对指定进程的nice的修改或读取功能，并返回进程最新的nice值及优先级prio。建议调用原型为 int mysetnice（pid_t pid， int flag， t nicevalue， void _user＊ prio， void _user＊nice）； 参数含义: id:进程ID flag:若值为0，则表示读取nice值；若值为1，则表示修改nice值。 nicevalue:为指定进程设置的新rice值。 prio、nice:指向进程当前优先级及nice值。 返回值:系统调用成功时返回0；失败时返回错误码 EFAULT。 （2）写一个简单的应用程序测试（1）中添加的系统调用。 （3）若程序中调用了 Linux的内核函数，要求深入阅读相关函数源码。 实验步骤 1. 实验环境 CentOS 7 64位，待编译的内核是 linux5.0.6.tar.xz。 虚拟机：VMware Workstation Pro 15 2. 下载并解压内核源码 官方网站上下载太慢，我在国内镜像源上找到了相同的内核版本，使用wget命令下载： wget https://mirrors.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.0.6.tar.xz 解压： xz -d linux-5.0.6.tar.xz tar xvf linux-5.0.6.tar 3. 配置安装编译环境 安装使用的是 yum命令，分别需要下载安装ncurses-devel,gcc,flex,bison,openssl-devel,elfutils-libelf-devel.。 yum -y install ncurses-devel yum -y install gcc yum -y install flex yum -y install bison yum -y install openssl-devel yum -y install elfutils-libelf-devel 4. 清理残留的文件 使用 make clean,和 make mrprproer都行 rm -f linux-5.0.6.tar cd linux-5.0.6 make clean 5. 配置内核 如果接下来是 make bzImage,就需要设置一下默认配置。但这里不用，直接 save 后 exit即可。 现在可以拍一张快照，因为接下来就要添加系统调用了。 6. 分配系统调用号 cd到arch/x86/entry/syscalls/syscall,打开系统调用表，根据规律添加一行： 335 common alternice __x64_sys_alternice 7. 申明系统调用服务例程原型 打开syscall.h添加一行 asmlinkage long sys_alternice(pid_t pid,int flag,int nicevalue,void __user*prio,void __user*nice); 8.实现系统调用服务例程 在sys.c文件里添加自己的系统调用。 SYSCALL_DEFINE5(alternice,pid_t,pid,int,flag,int,nicevalue,void __user *,prio,void __user *,nice) { int mprio=0; int mnice=0; int error=0; struct task_struct *p; printk(\"SetNice Start!\\n\"); for(p=&init_task;(p=next_task(p))!=&init_task;) { printk(\"This is For CYCLE! \"); if(p->pid==pid) { printk(\"Pointer GO!\"); if(flag==0) { printk(\"Flag == 0\\n\"); mprio=task_prio(p); mnice=task_nice(p); copy_to_user(prio,&mprio,sizeof(task_prio(p))); copy_to_user(nice,&mnice,sizeof(task_nice(p))); }else if(flag==1) { printk(\"Flag == 1\\n\"); set_user_nice(p,nicevalue); mprio=task_prio(p); mnice=task_nice(p); copy_to_user(nice,&mnice,sizeof(task_nice(p))); copy_to_user(prio,&mprio,sizeof(task_prio(p))); }else { printk(\"Flag == Nothing\\n\"); error=-EFAULT; } printk(\"Pointer END! \\n\"); return error; } } printk(\"The system calls BUG!!!!!!!\\n \"); error=-EFAULT; return error; } 9. 开始编译内核 我设置了虚拟机四个内核，所以输入make -j4 10. 模块编译安装与内核安装 make modules make modules_install make install 11.设置开机引导并重启 grub2-mkconfig -o /etc/grub2.cfg grub2-set-default 0 reboot 重启完输入uname -r查看当前内核版本。 发现更新到了5.0.6证明之前没有出错。 12.编写用户态程序测试系统调用 这一步可以在虚拟机外编写，然后复制到虚拟机内。于是我写了一个demo: mySyscall.c #include #include #include int main() { int nice = 0; int prio = 0; int flag = 0; int pid = 0; int nicevalue = 0; printf(\"请输入 PID:\"); scanf(\"%d\",&pid); printf(\"要改变这个进程的 nice 值吗? 是(1) , 否(0)\\n\"); printf(\"请输入:\"); scanf(\"%d\",&flag); if(flag==0) { if(syscall(335,pid,0,0,&prio,&nice)==0) { printf(\"当前 nice :%d,当前 prio :%d\",nice,prio); return 0; }else { printf(\"\\n系统调用错误，请检查系统调用！\\n\\n\"); return -1; } }else if(flag==1) { printf(\"请输入一个 nice 值:\\n\"); scanf(\"%d\",&nicevalue); syscall(335,pid,0,0,&prio,&nice); printf(\"\\n\\n原来的 nice 值:%d\\n\",nice); syscall(335,pid,1,nicevalue,&prio,&nice); printf(\"当前的 nice 值:%d\\n\\n\\n\",nice); return 0; }else { printf(\"输入无效！\\n\"); return 0; } } 编译完成： 接下来新建一个终端，输入 top 查看 一个例程（比如 top）的 NI 值 和 PR值。 可以看到 top 的 PID = 10796 PR = 20 NI = 0 我们运行 a.out修改一下： 成功将其修改： PID = 10796 PR = 22 NI = 2 再测试几次： 没有报错，检查 dmesg,查看 也无错误。成功调用了自己添加的系统调用！ 13. 实验总结 在每次实验时多记录一下每一步遇到的问题，详细调查每一步的原因。对于关键步骤有必要插入大量调试代码，虚心求教。最后的成功值得喜悦！ 实验进行了多次，我甚至自己写了一段小Shell，方便我多次实验，这对我在实验过程中有了莫大的帮助： cd ~ echo \"开始创建文件夹src\" >> /root/log.txt mkdir src echo \"创建文件夹 src 完成！\" >> /root/log.txt cd src echo \"正在下载内核 5.0.6，请确保你的机型是 CentOS7.X !\" >> /root/log.txt wget https://mirrors.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.0.6.tar.xz echo \"解压内核文件\" >> /root/log.txt xz -d linux-5.0.6.tar.xz tar xvf linux-5.0.6.tar echo \"解压完成！\" >> /root/log.txt echo \"开始下载所需安装包......\" >> /root/log.txt yum -y install ncurses-devel echo \"第一个完成...\" >> /root/log.txt yum -y install gcc echo \"第二个完成...\" >> /root/log.txt yum -y install flex echo \"第三个完成...\" >> /root/log.txt yum -y install bison echo \"第四个完成...\" >> /root/log.txt yum -y install openssl-devel echo \"第五个完成...\" >> /root/log.txt yum -y install elfutils-libelf-devel echo \"第六个完成...\" >> /root/log.txt echo \"安装工作结束！开始清理垃圾文件...\" >> /root/log.txt rm -f linux-5.0.6.tar cd linux-5.0.6 make clean echo \"清理完成！所有工作结束！！！感谢使用！\" >> /root/log.txt echo \"检查日志:\" history -c cat /root/log.txt make menuconfig cd ~/src/linux-5.0.6.tar/ make -j4 echo \"内核编译完成...\" >> /root/log.txt make modules echo \"模块编译完成...\" >> /root/log.txt make modules_install echo \"模块安装完成...\" >> /root/log.txt make install echo \"内核安装完成...\" >> /root/log.txt grub2-mkconfig -o /etc/grub2.cfg echo \"grub 引导完成...\" >> /root/log.txt grub2-set-default 0 history -c echo \"设置系统默认启动项完成...开始重启\" >> /root/log.txt reboot 最后全部完成啦！ "},"实验一/资料/nice.html":{"url":"实验一/资料/nice.html","title":"nice","keywords":"","body":"预备知识 我们可以通过ps -l看到当前正在运行的进程的详细信息: 其中, F：表示进程旗标，标识进程所拥有的权限，当我切换到root的时候为4表示拥有root权限，为1仅有fork()权限 S：表示进程当前的状态 R：当前正在运行(RUNNING) S：睡眠(SLEEP) D：不可中断 T：停止(STOP) Z：僵尸进程(ZOMBIE) UID：拥有该进程用户的用户ID PID：进程号 PPID：该进程父进程的进程好 C：CPU是用百分比 PRI：优先级 NI：nice值 ADDR/SZ/WCHAN：都与内存有关 TTY：登陆者的终端，和远程登陆脱不开干系 TIME：占用CPU时间 CMD：造成此进程的命令 我们把主要进程分为两种： ​ 1.I/O消耗型进程； ​ 2.处理器消耗型进程 当然也有既是I/O消耗型也是处理器消耗型的进程。 I/O消耗型进程如字面意思一样，轮到它的时候会把大部分时间消耗在I/O请求和等待I/O上，真正使用CPU的时间很少，处理器消耗性进程会把大部分时间用在使用CPU进行计算之类的，如果给这两种继承分配的时间片长度相等，就会体现出不公平。 同时我们想给处理器消耗型的进程多一些处理器时间，而给I/O消耗性进程少一些处理器时间，于是linux采取的不是简单的时间片调度算法，而是改进的优先级调度算法CFS。 Linux系统是抢占式的，什么概念呢，系统当前运行一个进程，但这个时候一个具有更高优先级的进程突然得到某种资源进入了就绪状态，然后他就来到cpu面前一脚踢开正在运行的进程（你奏凯，我先来）就抢夺了与CPU共度的美好时光。 CFS直接分配的不是时间片，而是CPU使用比，这个比例会受到nice值得影响，然后你懂，nice值低比重就高，nice高比重就低。 Q：Linux使用的CFS调度器，决定抢占时机的就是上面说的CPU使用比，注意！反而CPU使用比低的新进程会立刻投入运行，CPU使用比高的新进程会延迟运行，为什么呢？ A：因为CFS的出发点基于一个非常简单的概念：进程调度的效果应如同系统具备一个理想中的完美多任务处理器。在这种系统中每个进程将能获得1/n的处理器时间。就是说每个进程真正使用cpu的时间是一样的，包括I/O消耗型和处理器消耗型，以达到真正的公平，这就解释了刚才的问题，CPU使用比低的占用时间会不可避免的少于占用比高的进程，那我们只好让这个进程具有抢占能力，一就绪就可以抢占，这样子“看起来CPU使用比高了”（其实没变）“看起来CPU占用时间也和其他进程一样多了”（其实不多）。抢占能力高了，交互性相对而言就好了，你看你人打个游戏，交互性就得高，不然你按个键放个技能电脑不给你处理，你就坑队友了。 Linux对普通进程采用的是完全公平调度算法（CFS） Linux的进程调度并未使用直接均分时间片的方式，而是对优先级进行了改进，采用了两种不同的优先级范围，一种是nice值，范围是-20到+19，越大的nice值意味着更低的优先级，低nice值的进程会获得更多的处理器时间（按比例获得），第二种范围是实时优先级，其值是可配置的，默认情况下它的变化范围是从0到99，与nice值意义相反，越高的实时优先级数值意味着进程优先级越高，任何实时进程的优先级都高于普通进程 PS：nice值还会不断对old优先级进行更改，当然也可以设置nice的值，nice值给负值必须要用root nice值不是优先级，但会影响优先级 PRI(new)=PRI(old)+nice 时间片过长会导致人机交互欠佳，而时间片过短会导致大量的处理器时间浪费在进程的切换上，而且I/O消耗型进程和处理器消耗型进程之间的处理器时间的不公平之处也显现出来了 CFS并没有直接分配时间片到进程，而是将处理器的使用比划分给了进程，这个比例还会受到nice值的影响 CFS的做法是允许每个进程运行一段时间，循环轮转，选择运行最少的进程作为写一个运行进程，所以根据nice值的含义-占用处理器的百分比，来根据系统中全部可运行进程总数来根据所占比例的“时间片”运行 linux设计总是想分配给N个进程每个进程同样多的处理器运行时间，当N趋于无穷大的时候，按理说是可以分配给无限小的时间周期，但是这么做会很糟糕，我们也无法分配无限小的时间周期，虽然越小的时间周期可以带来更好的交互性，但还是带来不可接受的切换消耗，所以引入了一个目标延迟，来模拟无限小调度周期的近似值，现在假设目标延迟就是20ms，用它除以所有当前可以运行的进程数目就可以得到每个进程获得的时间片长度，当进程数无限大时候，每个进程分配的时间就趋于无限小，很好，进程切换又爆炸了，那么如果把最小值设为为1ms呢，进程数目再多我也保证每个进程在被强占之前获得1ms的运行时间，那么这个1ms就被称为最小粒度 然而最后影响CFS调度器调度进程的并不是优先级的nice值，因为CFS说了，我要让每个进程获得的处理器时间都是一样的（虽然不可能），所以有一个vruntime的值表示进程运行的虚拟时间（就是在处理器上跑的时间累加和），这个vruntime值越小，说明该进程应该被优先执行（或者获得更多的处理器时间片），因为他饿了。。。 一次调度间隔的虚拟运行时间=实际运行时间*（NICE_0_LOAD/权重）这就是nice值和vruntime之间的关系，其中，NICE_0_LOAD是nice为0时的权重 当nice值为0证明虚拟运行时间=实际运行时间 "},"实验一/资料/task_struct.html":{"url":"实验一/资料/task_struct.html","title":"task_struct","keywords":"","body":"Linux进程管理之task_struct结构体 进程是处于执行期的程序以及它所管理的资源（如打开的文件、挂起的信号、进程状态、地址空间等等）的总称。注意，程序并不是进程，实际上两个或多个进程不仅有可能执行同一程序，而且还有可能共享地址空间等资源。 ​ Linux内核通过一个被称为进程描述符的task_struct结构体来管理进程，这个结构体包含了一个进程所需的所有信息。它定义在linux-2.6.38.8/include/linux/sched.h文件中。 本文将尽力就task_struct结构体所有成员的用法进行简要说明。 ​ 1、进程状态 [cpp] view plain copy print? volatile long state; int exit_state; volatile long state; int exit_state; ​ state成员的可能取值如下： [cpp] view plain copy print? #define TASK_RUNNING 0 #define TASK_INTERRUPTIBLE 1 #define TASK_UNINTERRUPTIBLE 2 #define __TASK_STOPPED 4 #define __TASK_TRACED 8 / in tsk->exit_state / #define EXIT_ZOMBIE 16 #define EXIT_DEAD 32 / in tsk->state again / #define TASK_DEAD 64 #define TASK_WAKEKILL 128 #define TASK_WAKING 256 #define TASK_RUNNING 0 #define TASK_INTERRUPTIBLE 1 #define TASK_UNINTERRUPTIBLE 2 #define __TASK_STOPPED 4 #define __TASK_TRACED 8 /* in tsk->exit_state */ #define EXIT_ZOMBIE 16 #define EXIT_DEAD 32 /* in tsk->state again */ #define TASK_DEAD 64 #define TASK_WAKEKILL 128 #define TASK_WAKING 256 ​ 系统中的每个进程都必然处于以上所列进程状态中的一种。 ​ TASK_RUNNING表示进程要么正在执行，要么正要准备执行。 ​ TASK_INTERRUPTIBLE表示进程被阻塞（睡眠），直到某个条件变为真。条件一旦达成，进程的状态就被设置为TASK_RUNNING。 ​ TASK_UNINTERRUPTIBLE的意义与TASK_INTERRUPTIBLE类似，除了不能通过接受一个信号来唤醒以外。 ​ __TASK_STOPPED表示进程被停止执行。 ​ __TASK_TRACED表示进程被debugger等进程监视。 ​ EXIT_ZOMBIE表示进程的执行被终止，但是其父进程还没有使用wait()等系统调用来获知它的终止信息。 ​ EXIT_DEAD表示进程的最终状态。 ​ EXIT_ZOMBIE和EXIT_DEAD也可以存放在exit_state成员中。进程状态的切换过程和原因大致如下图（图片来自《Linux Kernel Development》）： ​ 2、进程标识符（PID） [cpp] view plain copy print? pid_t pid; pid_t tgid; pid_t pid; pid_t tgid; ​ 在CONFIG_BASE_SMALL配置为0的情况下，PID的取值范围是0到32767，即系统中的进程数最大为32768个。 [cpp] view plain copy print? / linux-2.6.38.8/include/linux/threads.h / #define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000) /* linux-2.6.38.8/include/linux/threads.h */ #define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000) ​ 在Linux系统中，一个线程组中的所有线程使用和该线程组的领头线程（该组中的第一个轻量级进程）相同的PID，并被存放在tgid成员中。只有线程组的领头线程的pid成员才会被设置为与tgid相同的值。注意，getpid()系统调用返回的是当前进程的tgid值而不是pid值。 ​ 3、进程内核栈 [cpp] view plain copy print? void *stack; void *stack; ​ 进程通过alloc_thread_info函数分配它的内核栈，通过free_thread_info函数释放所分配的内核栈。 [cpp] view plain copy print? / linux-2.6.38.8/kernel/fork.c / static inline struct thread_info *alloc_thread_info(struct task_struct *tsk) { #ifdef CONFIG_DEBUG_STACK_USAGE ​ gfp_t mask = GFP_KERNEL | __GFP_ZERO; #else ​ gfp_t mask = GFP_KERNEL; #endif ​ return (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER); } static inline void free_thread_info(struct thread_info *ti) { ​ free_pages((unsigned long)ti, THREAD_SIZE_ORDER); } /* linux-2.6.38.8/kernel/fork.c */ static inline struct thread_info *alloc_thread_info(struct task_struct *tsk) { #ifdef CONFIG_DEBUG_STACK_USAGE gfp_t mask = GFP_KERNEL | __GFP_ZERO; #else gfp_t mask = GFP_KERNEL; #endif return (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER); } static inline void free_thread_info(struct thread_info *ti) { free_pages((unsigned long)ti, THREAD_SIZE_ORDER); } ​ 其中，THREAD_SIZE_ORDER宏在linux-2.6.38.8/arch/arm/include/asm/thread_info.h文件中被定义为1，也就是说alloc_thread_info函数通过调用__get_free_pages函数分配2个页的内存（它的首地址是8192字节对齐的）。 ​ Linux内核通过thread_union联合体来表示进程的内核栈，其中THREAD_SIZE宏的大小为8192。 [cpp] view plain copy print? union thread_union { ​ struct thread_info thread_info; ​ unsigned long stack[THREAD_SIZE/sizeof(long)]; }; union thread_union { struct thread_info thread_info; unsigned long stack[THREAD_SIZE/sizeof(long)]; }; ​ 当进程从用户态切换到内核态时，进程的内核栈总是空的，所以ARM的sp寄存器指向这个栈的顶端。因此，内核能够轻易地通过sp寄存器获得当前正在CPU上运行的进程。 [cpp] view plain copy print? / linux-2.6.38.8/arch/arm/include/asm/current.h / static inline struct task_struct *get_current(void) { ​ return current_thread_info()->task; } #define current (get_current()) / linux-2.6.38.8/arch/arm/include/asm/thread_info.h / static inline struct thread_info *current_thread_info(void) { ​ register unsigned long sp asm (\"sp\"); ​ return (struct thread_info *)(sp & ~(THREAD_SIZE - 1)); } /* linux-2.6.38.8/arch/arm/include/asm/current.h */ static inline struct task_struct *get_current(void) { return current_thread_info()->task; } #define current (get_current()) /* linux-2.6.38.8/arch/arm/include/asm/thread_info.h */ static inline struct thread_info *current_thread_info(void) { register unsigned long sp asm (\"sp\"); return (struct thread_info *)(sp & ~(THREAD_SIZE - 1)); } ​ 进程内核栈与进程描述符的关系如下图： ​ 4、标记 [cpp] view plain copy print? unsigned int flags; / per process flags, defined below / unsigned int flags; /* per process flags, defined below */ ​ flags成员的可能取值如下： [cpp] view plain copy print? #define PF_KSOFTIRQD 0x00000001 / I am ksoftirqd / #define PF_STARTING 0x00000002 / being created / #define PF_EXITING 0x00000004 / getting shut down / #define PF_EXITPIDONE 0x00000008 / pi exit done on shut down / #define PF_VCPU 0x00000010 / I'm a virtual CPU / #define PF_WQ_WORKER 0x00000020 / I'm a workqueue worker / #define PF_FORKNOEXEC 0x00000040 / forked but didn't exec / #define PF_MCE_PROCESS 0x00000080 / process policy on mce errors / #define PF_SUPERPRIV 0x00000100 / used super-user privileges / #define PF_DUMPCORE 0x00000200 / dumped core / #define PF_SIGNALED 0x00000400 / killed by a signal / #define PF_MEMALLOC 0x00000800 / Allocating memory / #define PF_USED_MATH 0x00002000 / if unset the fpu must be initialized before use / #define PF_FREEZING 0x00004000 / freeze in progress. do not account to load / #define PF_NOFREEZE 0x00008000 / this thread should not be frozen / #define PF_FROZEN 0x00010000 / frozen for system suspend / #define PF_FSTRANS 0x00020000 / inside a filesystem transaction / #define PF_KSWAPD 0x00040000 / I am kswapd / #define PF_OOM_ORIGIN 0x00080000 / Allocating much memory to others / #define PF_LESS_THROTTLE 0x00100000 / Throttle me less: I clean memory / #define PF_KTHREAD 0x00200000 / I am a kernel thread / #define PF_RANDOMIZE 0x00400000 / randomize virtual address space / #define PF_SWAPWRITE 0x00800000 / Allowed to write to swap / #define PF_SPREAD_PAGE 0x01000000 / Spread page cache over cpuset / #define PF_SPREAD_SLAB 0x02000000 / Spread some slab caches over cpuset / #define PF_THREAD_BOUND 0x04000000 / Thread bound to specific cpu / #define PF_MCE_EARLY 0x08000000 / Early kill for mce process policy / #define PF_MEMPOLICY 0x10000000 / Non-default NUMA mempolicy / #define PF_MUTEX_TESTER 0x20000000 / Thread belongs to the rt mutex tester / #define PF_FREEZER_SKIP 0x40000000 / Freezer should not count it as freezable / #define PF_FREEZER_NOSIG 0x80000000 / Freezer won't send signals to it / #define PF_KSOFTIRQD 0x00000001 /* I am ksoftirqd */ #define PF_STARTING 0x00000002 /* being created */ #define PF_EXITING 0x00000004 /* getting shut down */ #define PF_EXITPIDONE 0x00000008 /* pi exit done on shut down */ #define PF_VCPU 0x00000010 /* I'm a virtual CPU */ #define PF_WQ_WORKER 0x00000020 /* I'm a workqueue worker */ #define PF_FORKNOEXEC 0x00000040 /* forked but didn't exec */ #define PF_MCE_PROCESS 0x00000080 /* process policy on mce errors */ #define PF_SUPERPRIV 0x00000100 /* used super-user privileges */ #define PF_DUMPCORE 0x00000200 /* dumped core */ #define PF_SIGNALED 0x00000400 /* killed by a signal */ #define PF_MEMALLOC 0x00000800 /* Allocating memory */ #define PF_USED_MATH 0x00002000 /* if unset the fpu must be initialized before use */ #define PF_FREEZING 0x00004000 /* freeze in progress. do not account to load */ #define PF_NOFREEZE 0x00008000 /* this thread should not be frozen */ #define PF_FROZEN 0x00010000 /* frozen for system suspend */ #define PF_FSTRANS 0x00020000 /* inside a filesystem transaction */ #define PF_KSWAPD 0x00040000 /* I am kswapd */ #define PF_OOM_ORIGIN 0x00080000 /* Allocating much memory to others */ #define PF_LESS_THROTTLE 0x00100000 /* Throttle me less: I clean memory */ #define PF_KTHREAD 0x00200000 /* I am a kernel thread */ #define PF_RANDOMIZE 0x00400000 /* randomize virtual address space */ #define PF_SWAPWRITE 0x00800000 /* Allowed to write to swap */ #define PF_SPREAD_PAGE 0x01000000 /* Spread page cache over cpuset */ #define PF_SPREAD_SLAB 0x02000000 /* Spread some slab caches over cpuset */ #define PF_THREAD_BOUND 0x04000000 /* Thread bound to specific cpu */ #define PF_MCE_EARLY 0x08000000 /* Early kill for mce process policy */ #define PF_MEMPOLICY 0x10000000 /* Non-default NUMA mempolicy */ #define PF_MUTEX_TESTER 0x20000000 /* Thread belongs to the rt mutex tester */ #define PF_FREEZER_SKIP 0x40000000 /* Freezer should not count it as freezable */ #define PF_FREEZER_NOSIG 0x80000000 /* Freezer won't send signals to it */ ​ 5、表示进程亲属关系的成员 [cpp] view plain copy print? struct task_struct *real_parent; /* real parent process */ struct task_struct *parent; /* recipient of SIGCHLD, wait4() reports */ struct list_head children; /* list of my children */ struct list_head sibling; /* linkage in my parent's children list */ struct task_struct *group_leader; /* threadgroup leader */ struct task_struct *real_parent; /* real parent process */ struct task_struct *parent; /* recipient of SIGCHLD, wait4() reports */ struct list_head children; /* list of my children */ struct list_head sibling; /* linkage in my parent's children list */ struct task_struct *group_leader; /* threadgroup leader */ ​ 在Linux系统中，所有进程之间都有着直接或间接地联系，每个进程都有其父进程，也可能有零个或多个子进程。拥有同一父进程的所有进程具有兄弟关系。 ​ real_parent指向其父进程，如果创建它的父进程不再存在，则指向PID为1的init进程。 ​ parent指向其父进程，当它终止时，必须向它的父进程发送信号。它的值通常与real_parent相同。 ​ children表示链表的头部，链表中的所有元素都是它的子进程。 ​ sibling用于把当前进程插入到兄弟链表中。 ​ group_leader指向其所在进程组的领头进程。 ​ 6、ptrace系统调用 [cpp] view plain copy print? unsigned int ptrace; struct list_head ptraced; struct list_head ptrace_entry; unsigned long ptrace_message; siginfo_t last_siginfo; / For ptrace use. */ ifdef CONFIG_HAVE_HW_BREAKPOINT atomic_t ptrace_bp_refcnt; endif unsigned int ptrace; struct list_head ptraced; struct list_head ptrace_entry; unsigned long ptrace_message; siginfo_t *last_siginfo; /* For ptrace use. */ #ifdef CONFIG_HAVE_HW_BREAKPOINT atomic_t ptrace_bp_refcnt; #endif ​ 成员ptrace被设置为0时表示不需要被跟踪，它的可能取值如下： [cpp] view plain copy print? / linux-2.6.38.8/include/linux/ptrace.h / #define PT_PTRACED 0x00000001 #define PT_DTRACE 0x00000002 / delayed trace (used on m68k, i386) / #define PT_TRACESYSGOOD 0x00000004 #define PT_PTRACE_CAP 0x00000008 / ptracer can follow suid-exec / #define PT_TRACE_FORK 0x00000010 #define PT_TRACE_VFORK 0x00000020 #define PT_TRACE_CLONE 0x00000040 #define PT_TRACE_EXEC 0x00000080 #define PT_TRACE_VFORK_DONE 0x00000100 #define PT_TRACE_EXIT 0x00000200 /* linux-2.6.38.8/include/linux/ptrace.h */ #define PT_PTRACED 0x00000001 #define PT_DTRACE 0x00000002 /* delayed trace (used on m68k, i386) */ #define PT_TRACESYSGOOD 0x00000004 #define PT_PTRACE_CAP 0x00000008 /* ptracer can follow suid-exec */ #define PT_TRACE_FORK 0x00000010 #define PT_TRACE_VFORK 0x00000020 #define PT_TRACE_CLONE 0x00000040 #define PT_TRACE_EXEC 0x00000080 #define PT_TRACE_VFORK_DONE 0x00000100 #define PT_TRACE_EXIT 0x00000200 ​ 7、Performance Event [cpp] view plain copy print? #ifdef CONFIG_PERF_EVENTS ​ struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts]; ​ struct mutex perf_event_mutex; ​ struct list_head perf_event_list; #endif #ifdef CONFIG_PERF_EVENTS struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts]; struct mutex perf_event_mutex; struct list_head perf_event_list; #endif ​ Performance Event是一款随 Linux 内核代码一同发布和维护的性能诊断工具。这些成员用于帮助PerformanceEvent分析进程的性能问题。 ​ 关于Performance Event工具的介绍可参考文章http://www.ibm.com/developerworks/cn/linux/l-cn-perf1/index.html?ca=drs-#major1和http://www.ibm.com/developerworks/cn/linux/l-cn-perf2/index.html?ca=drs-#major1。 ​ 8、进程调度 [cpp] view plain copy print? int prio, static_prio, normal_prio; unsigned int rt_priority; const struct sched_class *sched_class; struct sched_entity se; struct sched_rt_entity rt; unsigned int policy; cpumask_t cpus_allowed; int prio, static_prio, normal_prio; unsigned int rt_priority; const struct sched_class *sched_class; struct sched_entity se; struct sched_rt_entity rt; unsigned int policy; cpumask_t cpus_allowed; ​ 实时优先级范围是0到MAX_RT_PRIO-1（即99），而普通进程的静态优先级范围是从MAX_RT_PRIO到MAX_PRIO-1（即100到139）。值越大静态优先级越低。 [cpp] view plain copy print? / linux-2.6.38.8/include/linux/sched.h / #define MAX_USER_RT_PRIO 100 #define MAX_RT_PRIO MAX_USER_RT_PRIO #define MAX_PRIO (MAX_RT_PRIO + 40) #define DEFAULT_PRIO (MAX_RT_PRIO + 20) /* linux-2.6.38.8/include/linux/sched.h */ #define MAX_USER_RT_PRIO 100 #define MAX_RT_PRIO MAX_USER_RT_PRIO #define MAX_PRIO (MAX_RT_PRIO + 40) #define DEFAULT_PRIO (MAX_RT_PRIO + 20) ​ static_prio用于保存静态优先级，可以通过nice系统调用来进行修改。 ​ rt_priority用于保存实时优先级。 ​ normal_prio的值取决于静态优先级和调度策略。 ​ prio用于保存动态优先级。 ​ policy表示进程的调度策略，目前主要有以下五种： [cpp] view plain copy print? #define SCHED_NORMAL 0 #define SCHED_FIFO 1 #define SCHED_RR 2 #define SCHED_BATCH 3 / SCHED_ISO: reserved but not implemented yet / #define SCHED_IDLE 5 #define SCHED_NORMAL 0 #define SCHED_FIFO 1 #define SCHED_RR 2 #define SCHED_BATCH 3 /* SCHED_ISO: reserved but not implemented yet */ #define SCHED_IDLE 5 ​ SCHED_NORMAL用于普通进程，通过CFS调度器实现。SCHED_BATCH用于非交互的处理器消耗型进程。SCHED_IDLE是在系统负载很低时使用。 ​ SCHED_FIFO（先入先出调度算法）和SCHED_RR（轮流调度算法）都是实时调度策略。 ​ sched_class结构体表示调度类，目前内核中有实现以下四种： [cpp] view plain copy print? / linux-2.6.38.8/kernel/sched_fair.c / static const struct sched_class fair_sched_class; / linux-2.6.38.8/kernel/sched_rt.c / static const struct sched_class rt_sched_class; / linux-2.6.38.8/kernel/sched_idletask.c / static const struct sched_class idle_sched_class; / linux-2.6.38.8/kernel/sched_stoptask.c / static const struct sched_class stop_sched_class; /* linux-2.6.38.8/kernel/sched_fair.c */ static const struct sched_class fair_sched_class; /* linux-2.6.38.8/kernel/sched_rt.c */ static const struct sched_class rt_sched_class; /* linux-2.6.38.8/kernel/sched_idletask.c */ static const struct sched_class idle_sched_class; /* linux-2.6.38.8/kernel/sched_stoptask.c */ static const struct sched_class stop_sched_class; ​ se和rt都是调用实体，一个用于普通进程，一个用于实时进程，每个进程都有其中之一的实体。 ​ cpus_allowed用于控制进程可以在哪里处理器上运行。 ​ 9、进程地址空间 [cpp] view plain copy print? ​ struct mm_struct *mm, *active_mm; #ifdef CONFIG_COMPAT_BRK ​ unsigned brk_randomized:1; #endif #if defined(SPLIT_RSS_COUNTING) ​ struct task_rss_stat rss_stat; #endif struct mm_struct *mm, *active_mm; #ifdef CONFIG_COMPAT_BRK unsigned brk_randomized:1; #endif #if defined(SPLIT_RSS_COUNTING) struct task_rss_stat rss_stat; #endif ​ mm指向进程所拥有的内存描述符，而active_mm指向进程运行时所使用的内存描述符。对于普通进程而言，这两个指针变量的值相同。但是，内核线程不拥有任何内存描述符，所以它们的mm成员总是为NULL。当内核线程得以运行时，它的active_mm成员被初始化为前一个运行进程的active_mm值。 ​ brk_randomized的用法在http://lkml.indiana.edu/hypermail/[Linux](http://lib.csdn.net/base/linux)/kernel/1104.1/00196.html上有介绍，用来确定对随机堆内存的探测。 ​ rss_stat用来记录缓冲信息。 ​ 10、判断标志 [cpp] view plain copy print? int exit_code, exit_signal; int pdeath_signal; / The signal sent when the parent dies / / ??? / unsigned int personality; unsigned did_exec:1; unsigned in_execve:1; /* Tell the LSMs that the process is doing an ​ * execve */ unsigned in_iowait:1; / Revert to default priority/policy when forking / unsigned sched_reset_on_fork:1; int exit_code, exit_signal; int pdeath_signal; /* The signal sent when the parent dies */ /* ??? */ unsigned int personality; unsigned did_exec:1; unsigned in_execve:1; /* Tell the LSMs that the process is doing an * execve */ unsigned in_iowait:1; /* Revert to default priority/policy when forking */ unsigned sched_reset_on_fork:1; ​ exit_code用于设置进程的终止代号，这个值要么是_exit()或exit_group()系统调用参数（正常终止），要么是由内核提供的一个错误代号（异常终止）。 ​ exit_signal被置为-1时表示是某个线程组中的一员。只有当线程组的最后一个成员终止时，才会产生一个信号，以通知线程组的领头进程的父进程。 ​ pdeath_signal用于判断父进程终止时发送信号。 ​ personality用于处理不同的ABI，它的可能取值如下： [cpp] view plain copy print? enum { ​ PER_LINUX = 0x0000, ​ PER_LINUX_32BIT = 0x0000 | ADDR_LIMIT_32BIT, ​ PER_LINUX_FDPIC = 0x0000 | FDPIC_FUNCPTRS, ​ PER_SVR4 = 0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO, ​ PER_SVR3 = 0x0002 | STICKY_TIMEOUTS | SHORT_INODE, ​ PER_SCOSVR3 = 0x0003 | STICKY_TIMEOUTS | ​ WHOLE_SECONDS | SHORT_INODE, ​ PER_OSR5 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS, ​ PER_WYSEV386 = 0x0004 | STICKY_TIMEOUTS | SHORT_INODE, ​ PER_ISCR4 = 0x0005 | STICKY_TIMEOUTS, ​ PER_BSD = 0x0006, ​ PER_SUNOS = 0x0006 | STICKY_TIMEOUTS, ​ PER_XENIX = 0x0007 | STICKY_TIMEOUTS | SHORT_INODE, ​ PER_LINUX32 = 0x0008, ​ PER_LINUX32_3GB = 0x0008 | ADDR_LIMIT_3GB, ​ PER_IRIX32 = 0x0009 | STICKY_TIMEOUTS,/ IRIX5 32-bit / ​ PER_IRIXN32 = 0x000a | STICKY_TIMEOUTS,/ IRIX6 new 32-bit / ​ PER_IRIX64 = 0x000b | STICKY_TIMEOUTS,/ IRIX6 64-bit / ​ PER_RISCOS = 0x000c, ​ PER_SOLARIS = 0x000d | STICKY_TIMEOUTS, ​ PER_UW7 = 0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO, ​ PER_OSF4 = 0x000f, / OSF/1 v4 / ​ PER_HPUX = 0x0010, ​ PER_MASK = 0x00ff, }; enum { PER_LINUX = 0x0000, PER_LINUX_32BIT = 0x0000 | ADDR_LIMIT_32BIT, PER_LINUX_FDPIC = 0x0000 | FDPIC_FUNCPTRS, PER_SVR4 = 0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO, PER_SVR3 = 0x0002 | STICKY_TIMEOUTS | SHORT_INODE, PER_SCOSVR3 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS | SHORT_INODE, PER_OSR5 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS, PER_WYSEV386 = 0x0004 | STICKY_TIMEOUTS | SHORT_INODE, PER_ISCR4 = 0x0005 | STICKY_TIMEOUTS, PER_BSD = 0x0006, PER_SUNOS = 0x0006 | STICKY_TIMEOUTS, PER_XENIX = 0x0007 | STICKY_TIMEOUTS | SHORT_INODE, PER_LINUX32 = 0x0008, PER_LINUX32_3GB = 0x0008 | ADDR_LIMIT_3GB, PER_IRIX32 = 0x0009 | STICKY_TIMEOUTS,/* IRIX5 32-bit */ PER_IRIXN32 = 0x000a | STICKY_TIMEOUTS,/* IRIX6 new 32-bit */ PER_IRIX64 = 0x000b | STICKY_TIMEOUTS,/* IRIX6 64-bit */ PER_RISCOS = 0x000c, PER_SOLARIS = 0x000d | STICKY_TIMEOUTS, PER_UW7 = 0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO, PER_OSF4 = 0x000f, /* OSF/1 v4 */ PER_HPUX = 0x0010, PER_MASK = 0x00ff, }; ​ did_exec用于记录进程代码是否被execve()函数所执行。 ​ in_execve用于通知LSM是否被do_execve()函数所调用。详见补丁说明：http://lkml.indiana.edu/hypermail/linux/kernel/0901.1/00014.html。 ​ in_iowait用于判断是否进行iowait计数。 ​ sched_reset_on_fork用于判断是否恢复默认的优先级或调度策略。 ​ 11、时间 [cpp] view plain copy print? ​ cputime_t utime, stime, utimescaled, stimescaled; ​ cputime_t gtime; #ifndef CONFIG_VIRT_CPU_ACCOUNTING ​ cputime_t prev_utime, prev_stime; #endif ​ unsigned long nvcsw, nivcsw; / context switch counts / ​ struct timespec start_time; /* monotonic time */ ​ struct timespec real_start_time; /* boot based time */ ​ struct task_cputime cputime_expires; ​ struct list_head cpu_timers[3]; #ifdef CONFIG_DETECT_HUNG_TASK / hung task detection / ​ unsigned long last_switch_count; #endif cputime_t utime, stime, utimescaled, stimescaled; cputime_t gtime; #ifndef CONFIG_VIRT_CPU_ACCOUNTING cputime_t prev_utime, prev_stime; #endif unsigned long nvcsw, nivcsw; /* context switch counts */ struct timespec start_time; /* monotonic time */ struct timespec real_start_time; /* boot based time */ struct task_cputime cputime_expires; struct list_head cpu_timers[3]; #ifdef CONFIG_DETECT_HUNG_TASK /* hung task detection */ unsigned long last_switch_count; #endif ​ utime/stime用于记录进程在用户态/内核态下所经过的节拍数（定时器）。prev_utime/prev_stime是先前的运行时间，请参考补丁说明http://lkml.indiana.edu/hypermail/linux/kernel/1003.3/02431.html。 ​ utimescaled/stimescaled也是用于记录进程在用户态/内核态的运行时间，但它们以处理器的频率为刻度。 ​ gtime是以节拍计数的虚拟机运行时间（guest time）。 ​ nvcsw/nivcsw是自愿（voluntary）/非自愿（involuntary）上下文切换计数。last_switch_count是nvcsw和nivcsw的总和。 ​ start_time和real_start_time都是进程创建时间，real_start_time还包含了进程睡眠时间，常用于/proc/pid/stat，补丁说明请参考http://lkml.indiana.edu/hypermail/linux/kernel/0705.0/2094.html。 ​ cputime_expires用来统计进程或进程组被跟踪的处理器时间，其中的三个成员对应着cpu_timers[3]的三个链表。 ​ 12、信号处理 [cpp] view plain copy print? / signal handlers / ​ struct signal_struct *signal; ​ struct sighand_struct *sighand; ​ sigset_t blocked, real_blocked; ​ sigset_t saved_sigmask; / restored if set_restore_sigmask() was used / ​ struct sigpending pending; ​ unsigned long sas_ss_sp; ​ size_t sas_ss_size; ​ int (notifier)(**void \\priv);** ​ void *notifier_data; ​ sigset_t *notifier_mask; /* signal handlers */ struct signal_struct *signal; struct sighand_struct *sighand; sigset_t blocked, real_blocked; sigset_t saved_sigmask; /* restored if set_restore_sigmask() was used */ struct sigpending pending; unsigned long sas_ss_sp; size_t sas_ss_size; int (*notifier)(void *priv); void *notifier_data; sigset_t *notifier_mask; ​ signal指向进程的信号描述符。 ​ sighand指向进程的信号处理程序描述符。 ​ blocked表示被阻塞信号的掩码，real_blocked表示临时掩码。 ​ pending存放私有挂起信号的数据结构。 ​ sas_ss_sp是信号处理程序备用堆栈的地址，sas_ss_size表示堆栈的大小。 ​ 设备驱动程序常用notifier指向的函数来阻塞进程的某些信号（notifier_mask是这些信号的位掩码），notifier_data指的是notifier所指向的函数可能使用的数据。 ​ 13、其他 ​ （1）、用于保护资源分配或释放的自旋锁 [cpp] view plain copy print? /* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, * mempolicy */ ​ spinlock_t alloc_lock; /* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, * mempolicy */ spinlock_t alloc_lock; ​ （2）、进程描述符使用计数，被置为2时，表示进程描述符正在被使用而且其相应的进程处于活动状态。 [cpp] view plain copy print? atomic_t usage; atomic_t usage; ​ （3）、用于表示获取大内核锁的次数，如果进程未获得过锁，则置为-1。 [cpp] view plain copy print? int lock_depth; / BKL lock depth / int lock_depth; /* BKL lock depth */ ​ （4）、在SMP上帮助实现无加锁的进程切换（unlocked context switches） [cpp] view plain copy print? #ifdef CONFIG_SMP #ifdef __ARCH_WANT_UNLOCKED_CTXSW ​ int oncpu; #endif #endif #ifdef CONFIG_SMP #ifdef __ARCH_WANT_UNLOCKED_CTXSW int oncpu; #endif #endif ​ （5）、preempt_notifier结构体链表 [cpp] view plain copy print? #ifdef CONFIG_PREEMPT_NOTIFIERS ​ / list of struct preempt_notifier: / ​ struct hlist_head preempt_notifiers; #endif #ifdef CONFIG_PREEMPT_NOTIFIERS /* list of struct preempt_notifier: */ struct hlist_head preempt_notifiers; #endif ​ （6）、FPU使用计数 [cpp] view plain copy print? unsigned char fpu_counter; unsigned char fpu_counter; ​ （7）、blktrace是一个针对Linux内核中块设备I/O层的跟踪工具。 [cpp] view plain copy print? #ifdef CONFIG_BLK_DEV_IO_TRACE ​ unsigned int btrace_seq; #endif #ifdef CONFIG_BLK_DEV_IO_TRACE unsigned int btrace_seq; #endif ​ （8）、RCU同步原语 [cpp] view plain copy print? #ifdef CONFIG_PREEMPT_RCU ​ int rcu_read_lock_nesting; ​ char rcu_read_unlock_special; ​ struct list_head rcu_node_entry; #endif / #ifdef CONFIG_PREEMPT_RCU / #ifdef CONFIG_TREE_PREEMPT_RCU ​ struct rcu_node *rcu_blocked_node; #endif / #ifdef CONFIG_TREE_PREEMPT_RCU / #ifdef CONFIG_RCU_BOOST ​ struct rt_mutex *rcu_boost_mutex; #endif / #ifdef CONFIG_RCU_BOOST / #ifdef CONFIG_PREEMPT_RCU int rcu_read_lock_nesting; char rcu_read_unlock_special; struct list_head rcu_node_entry; #endif /* #ifdef CONFIG_PREEMPT_RCU */ #ifdef CONFIG_TREE_PREEMPT_RCU struct rcu_node *rcu_blocked_node; #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */ #ifdef CONFIG_RCU_BOOST struct rt_mutex *rcu_boost_mutex; #endif /* #ifdef CONFIG_RCU_BOOST */ ​ （9）、用于调度器统计进程的运行信息 [cpp] view plain copy print? #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) ​ struct sched_info sched_info; #endif #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) struct sched_info sched_info; #endif ​ （10）、用于构建进程链表 [cpp] view plain copy print? struct list_head tasks; struct list_head tasks; ​ （11）、to limit pushing to one attempt [cpp] view plain copy print? #ifdef CONFIG_SMP ​ struct plist_node pushable_tasks; #endif #ifdef CONFIG_SMP struct plist_node pushable_tasks; #endif ​ 补丁说明请参考：http://lkml.indiana.edu/hypermail/linux/kernel/0808.3/0503.html ​ （12）、防止内核堆栈溢出 [cpp] view plain copy print? #ifdef CONFIG_CC_STACKPROTECTOR ​ / Canary value for the -fstack-protector gcc feature / ​ unsigned long stack_canary; #endif #ifdef CONFIG_CC_STACKPROTECTOR /* Canary value for the -fstack-protector gcc feature */ unsigned long stack_canary; #endif ​ 在GCC编译内核时，需要加上-fstack-protector选项。 ​ （13）、PID散列表和链表 [cpp] view plain copy print? / PID/PID hash table linkage. / struct pid_link pids[PIDTYPE_MAX]; struct list_head thread_group; //线程组中所有进程的链表 /* PID/PID hash table linkage. */ struct pid_link pids[PIDTYPE_MAX]; struct list_head thread_group; //线程组中所有进程的链表 ​ （14）、do_fork函数 [cpp] view plain copy print? struct completion *vfork_done; /* for vfork() */ int __user set_child_tid; / CLONE_CHILD_SETTID */ int __user clear_child_tid; / CLONE_CHILD_CLEARTID */ struct completion *vfork_done; /* for vfork() */ int __user *set_child_tid; /* CLONE_CHILD_SETTID */ int __user *clear_child_tid; /* CLONE_CHILD_CLEARTID */ ​ 在执行do_fork()时，如果给定特别标志，则vfork_done会指向一个特殊地址。 ​ 如果copy_process函数的clone_flags参数的值被置为CLONE_CHILD_SETTID或CLONE_CHILD_CLEARTID，则会把child_tidptr参数的值分别复制到set_child_tid和clear_child_tid成员。这些标志说明必须改变子进程用户态地址空间的child_tidptr所指向的变量的值。 ​ （15）、缺页统计 [cpp] view plain copy print? / mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific / ​ unsigned long min_flt, maj_flt; /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */ unsigned long min_flt, maj_flt; ​ （16）、进程权能 [cpp] view plain copy print? const struct cred __rcu *real_cred; /* objective and real subjective task ​ * credentials (COW) */ const struct cred __rcu *cred; /* effective (overridable) subjective task ​ * credentials (COW) */ struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */ const struct cred __rcu *real_cred; /* objective and real subjective task * credentials (COW) */ const struct cred __rcu *cred; /* effective (overridable) subjective task * credentials (COW) */ struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */ ​ （17）、相应的程序名 [cpp] view plain copy print? char comm[TASK_COMM_LEN]; char comm[TASK_COMM_LEN]; ​ （18）、文件 [cpp] view plain copy print? / file system info / ​ int link_count, total_link_count; / filesystem information / ​ struct fs_struct *fs; / open file information / ​ struct files_struct *files; /* file system info */ int link_count, total_link_count; /* filesystem information */ struct fs_struct *fs; /* open file information */ struct files_struct *files; ​ fs用来表示进程与文件系统的联系，包括当前目录和根目录。 ​ files表示进程当前打开的文件。 ​ （19）、进程通信（SYSVIPC） [cpp] view plain copy print? #ifdef CONFIG_SYSVIPC / ipc stuff / ​ struct sysv_sem sysvsem; #endif #ifdef CONFIG_SYSVIPC /* ipc stuff */ struct sysv_sem sysvsem; #endif ​ （20）、处理器特有数据 [cpp] view plain copy print? / CPU-specific state of this task / ​ struct thread_struct thread; /* CPU-specific state of this task */ struct thread_struct thread; ​ （21）、命名空间 [cpp] view plain copy print? / namespaces / ​ struct nsproxy *nsproxy; /* namespaces */ struct nsproxy *nsproxy; ​ （22）、进程审计 [cpp] view plain copy print? ​ struct audit_context *audit_context; #ifdef CONFIG_AUDITSYSCALL ​ uid_t loginuid; ​ unsigned int sessionid; #endif struct audit_context *audit_context; #ifdef CONFIG_AUDITSYSCALL uid_t loginuid; unsigned int sessionid; #endif ​ （23）、secure computing [cpp] view plain copy print? seccomp_t seccomp; seccomp_t seccomp; ​ （24）、用于copy_process函数使用CLONE_PARENT 标记时 [cpp] view plain copy print? / Thread group tracking / ​ u32 parent_exec_id; ​ u32 self_exec_id; /* Thread group tracking */ u32 parent_exec_id; u32 self_exec_id; ​ （25）、中断 [cpp] view plain copy print? #ifdef CONFIG_GENERIC_HARDIRQS ​ / IRQ handler threads / ​ struct irqaction *irqaction; #endif #ifdef CONFIG_TRACE_IRQFLAGS ​ unsigned int irq_events; ​ unsigned long hardirq_enable_ip; ​ unsigned long hardirq_disable_ip; ​ unsigned int hardirq_enable_event; ​ unsigned int hardirq_disable_event; ​ int hardirqs_enabled; ​ int hardirq_context; ​ unsigned long softirq_disable_ip; ​ unsigned long softirq_enable_ip; ​ unsigned int softirq_disable_event; ​ unsigned int softirq_enable_event; ​ int softirqs_enabled; ​ int softirq_context; #endif #ifdef CONFIG_GENERIC_HARDIRQS /* IRQ handler threads */ struct irqaction *irqaction; #endif #ifdef CONFIG_TRACE_IRQFLAGS unsigned int irq_events; unsigned long hardirq_enable_ip; unsigned long hardirq_disable_ip; unsigned int hardirq_enable_event; unsigned int hardirq_disable_event; int hardirqs_enabled; int hardirq_context; unsigned long softirq_disable_ip; unsigned long softirq_enable_ip; unsigned int softirq_disable_event; unsigned int softirq_enable_event; int softirqs_enabled; int softirq_context; #endif ​ （26）、task_rq_lock函数所使用的锁 [cpp] view plain copy print? / Protection of the PI data structures: / raw_spinlock_t pi_lock; /* Protection of the PI data structures: */ raw_spinlock_t pi_lock; ​ （27）、基于PI协议的等待互斥锁，其中PI指的是priority inheritance（优先级继承） [cpp] view plain copy print? #ifdef CONFIG_RT_MUTEXES ​ / PI waiters blocked on a rt_mutex held by this task / ​ struct plist_head pi_waiters; ​ / Deadlock detection and priority inheritance handling / ​ struct rt_mutex_waiter *pi_blocked_on; #endif #ifdef CONFIG_RT_MUTEXES /* PI waiters blocked on a rt_mutex held by this task */ struct plist_head pi_waiters; /* Deadlock detection and priority inheritance handling */ struct rt_mutex_waiter *pi_blocked_on; #endif ​ （28）、死锁检测 [cpp] view plain copy print? #ifdef CONFIG_DEBUG_MUTEXES ​ / mutex deadlock detection / ​ struct mutex_waiter *blocked_on; #endif #ifdef CONFIG_DEBUG_MUTEXES /* mutex deadlock detection */ struct mutex_waiter *blocked_on; #endif ​ （29）、lockdep，参见内核说明文档linux-2.6.38.8/Documentation/lockdep-design.txt [cpp] view plain copy print? #ifdef CONFIG_LOCKDEP # define MAX_LOCK_DEPTH 48UL ​ u64 curr_chain_key; ​ int lockdep_depth; ​ unsigned int lockdep_recursion; ​ struct held_lock held_locks[MAX_LOCK_DEPTH]; ​ gfp_t lockdep_reclaim_gfp; #endif #ifdef CONFIG_LOCKDEP # define MAX_LOCK_DEPTH 48UL u64 curr_chain_key; int lockdep_depth; unsigned int lockdep_recursion; struct held_lock held_locks[MAX_LOCK_DEPTH]; gfp_t lockdep_reclaim_gfp; #endif ​ （30）、JFS文件系统 [cpp] view plain copy print? / journalling filesystem info / ​ void *journal_info; /* journalling filesystem info */ void *journal_info; ​ （31）、块设备链表 [cpp] view plain copy print? / stacked block device info / ​ struct bio_list *bio_list; /* stacked block device info */ struct bio_list *bio_list; ​ （32）、内存回收 [cpp] view plain copy print? struct reclaim_state *reclaim_state; struct reclaim_state *reclaim_state; ​ （33）、存放块设备I/O数据流量信息 [cpp] view plain copy print? struct backing_dev_info *backing_dev_info; struct backing_dev_info *backing_dev_info; ​ （34）、I/O调度器所使用的信息 [cpp] view plain copy print? struct io_context *io_context; struct io_context *io_context; ​ （35）、记录进程的I/O计数 [cpp] view plain copy print? struct task_io_accounting ioac; if defined(CONFIG_TASK_XACCT) u64 acct_rss_mem1; / accumulated rss usage / u64 acct_vm_mem1; / accumulated virtual memory usage / cputime_t acct_timexpd; / stime + utime since last update / endif struct task_io_accounting ioac; #if defined(CONFIG_TASK_XACCT) u64 acct_rss_mem1; /* accumulated rss usage */ u64 acct_vm_mem1; /* accumulated virtual memory usage */ cputime_t acct_timexpd; /* stime + utime since last update */ #endif ​ 在Ubuntu 11.04上，执行cat获得进程1的I/O计数如下： [cpp] view plain copy print? $ sudo cat /proc/1/io $ sudo cat /proc/1/io [cpp] view plain copy print? rchar: 164258906 wchar: 455212837 syscr: 388847 syscw: 92563 read_bytes: 439251968 write_bytes: 14143488 cancelled_write_bytes: 2134016 rchar: 164258906 wchar: 455212837 syscr: 388847 syscw: 92563 read_bytes: 439251968 write_bytes: 14143488 cancelled_write_bytes: 2134016 ​ 输出的数据项刚好是task_io_accounting结构体的所有成员。 ​ （36）、CPUSET功能 [cpp] view plain copy print? #ifdef CONFIG_CPUSETS ​ nodemask_t mems_allowed; / Protected by alloc_lock / ​ int mems_allowed_change_disable; ​ int cpuset_mem_spread_rotor; ​ int cpuset_slab_spread_rotor; #endif #ifdef CONFIG_CPUSETS nodemask_t mems_allowed; /* Protected by alloc_lock */ int mems_allowed_change_disable; int cpuset_mem_spread_rotor; int cpuset_slab_spread_rotor; #endif ​ （37）、Control Groups [cpp] view plain copy print? #ifdef CONFIG_CGROUPS ​ / Control Group info protected by css_set_lock / ​ struct css_set __rcu *cgroups; ​ / cg_list protected by css_set_lock and tsk->alloc_lock / ​ struct list_head cg_list; #endif #ifdef CONFIG_CGROUP_MEM_RES_CTLR / memcg uses this to do batch job / ​ struct memcg_batch_info { ​ int do_batch; / incremented when batch uncharge started / ​ struct mem_cgroup *memcg; /* target memcg of uncharge */ ​ unsigned long bytes; / uncharged usage / ​ unsigned long memsw_bytes; / uncharged mem+swap usage / ​ } memcg_batch; #endif #ifdef CONFIG_CGROUPS /* Control Group info protected by css_set_lock */ struct css_set __rcu *cgroups; /* cg_list protected by css_set_lock and tsk->alloc_lock */ struct list_head cg_list; #endif #ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */ struct memcg_batch_info { int do_batch; /* incremented when batch uncharge started */ struct mem_cgroup *memcg; /* target memcg of uncharge */ unsigned long bytes; /* uncharged usage */ unsigned long memsw_bytes; /* uncharged mem+swap usage */ } memcg_batch; #endif ​ （38）、futex同步机制 [cpp] view plain copy print? #ifdef CONFIG_FUTEX ​ struct robust_list_head __user *robust_list; #ifdef CONFIG_COMPAT ​ struct compat_robust_list_head __user *compat_robust_list; #endif ​ struct list_head pi_state_list; ​ struct futex_pi_state *pi_state_cache; #endif #ifdef CONFIG_FUTEX struct robust_list_head __user *robust_list; #ifdef CONFIG_COMPAT struct compat_robust_list_head __user *compat_robust_list; #endif struct list_head pi_state_list; struct futex_pi_state *pi_state_cache; #endif ​ （39）、非一致内存访问（NUMA Non-Uniform Memory Access） [cpp] view plain copy print? #ifdef CONFIG_NUMA ​ struct mempolicy *mempolicy; /* Protected by alloc_lock */ ​ short il_next; #endif #ifdef CONFIG_NUMA struct mempolicy *mempolicy; /* Protected by alloc_lock */ short il_next; #endif ​ （40）、文件系统互斥资源 [cpp] view plain copy print? atomic_t fs_excl; / holding fs exclusive resources / atomic_t fs_excl; /* holding fs exclusive resources */ ​ （41）、RCU链表 [cpp] view plain copy print? struct rcu_head rcu; struct rcu_head rcu; ​ （42）、管道 [cpp] view plain copy print? struct pipe_inode_info *splice_pipe; struct pipe_inode_info *splice_pipe; ​ （43）、延迟计数 [cpp] view plain copy print? #ifdef CONFIG_TASK_DELAY_ACCT ​ struct task_delay_info *delays; #endif #ifdef CONFIG_TASK_DELAY_ACCT struct task_delay_info *delays; #endif ​ （44）、fault injection，参考内核说明文件linux-2.6.38.8/Documentation/fault-injection/fault-injection.txt [cpp] view plain copy print? #ifdef CONFIG_FAULT_INJECTION ​ int make_it_fail; #endif #ifdef CONFIG_FAULT_INJECTION int make_it_fail; #endif ​ （45）、FLoating proportions [cpp] view plain copy print? struct prop_local_single dirties; struct prop_local_single dirties; ​ （46）、Infrastructure for displayinglatency [cpp] view plain copy print? #ifdef CONFIG_LATENCYTOP ​ int latency_record_count; ​ struct latency_record latency_record[LT_SAVECOUNT]; #endif #ifdef CONFIG_LATENCYTOP int latency_record_count; struct latency_record latency_record[LT_SAVECOUNT]; #endif ​ （47）、time slack values，常用于poll和select函数 [cpp] view plain copy print? unsigned long timer_slack_ns; unsigned long default_timer_slack_ns; unsigned long timer_slack_ns; unsigned long default_timer_slack_ns; ​ （48）、socket控制消息（control message） [cpp] view plain copy print? struct list_head *scm_work_list; struct list_head *scm_work_list; ​ （49）、ftrace跟踪器 [cpp] view plain copy print? #ifdef CONFIG_FUNCTION_GRAPH_TRACER ​ / Index of current stored address in ret_stack / ​ int curr_ret_stack; ​ / Stack of return addresses for return function tracing / ​ struct ftrace_ret_stack *ret_stack; ​ / time stamp for last schedule / ​ unsigned long long ftrace_timestamp; ​ /* ​ * Number of functions that haven't been traced ​ * because of depth overrun. ​ */ ​ atomic_t trace_overrun; ​ / Pause for the tracing / ​ atomic_t tracing_graph_pause; #endif #ifdef CONFIG_TRACING ​ / state flags for use by tracers / ​ unsigned long trace; ​ / bitmask of trace recursion / ​ unsigned long trace_recursion; #endif / CONFIG_TRACING / "},"实验二/过程.html":{"url":"实验二/过程.html","title":"实验二","keywords":"","body":"实验二 ： Linux内核模块编程 （1）设计一个模块，要求列出系统中所有内核线程的程序名、PID、进程状态、进程优先级、父进程的PID。 cd ~ mkdir A B cd A vim show_all_kernel_thread.c 复制粘贴show_all_kernel_thread.c的代码 vim Makefile 复制粘贴Makefile1的代码 make insmod show_all_kernel_thread.ko dmesg 新开一个终端 ps -aux 对比两个终端的输出结果 （2）设计一个带参数的模块，其参数为某个进程的PID号，模块的功能是列出该进程的家族信息，包括父进程、兄弟进程和子进程的程序名、PID号及进程状态。 cd ~ cd B vim show_task_family.c "}}